import requests
from bs4 import BeautifulSoup
import csv

def naÄti_url(url):
    """NaÄte HTML obsah danÃ© strÃ¡nky."""
    odpovÄ›Ä = requests.get(url)
    if odpovÄ›Ä.status_code == 200:
        return BeautifulSoup(odpovÄ›Ä.content, "html.parser")
    else:
        raise ConnectionError(f"âŒ Chyba pÅ™i naÄÃ­tÃ¡nÃ­ strÃ¡nky {url} (HTTP {odpovÄ›Ä.status_code})")

def vyÄisti_ÄÃ­slo(hodnota):
    """PÅ™evede ÄÃ­slo z formÃ¡tu '1 000,00' na float/int."""
    hodnota = hodnota.replace("\xa0", "").replace(",", ".").strip()
    return float(hodnota) if "." in hodnota else int(hodnota)

def extrahuj_odkazy_na_obce(hlavnÃ­_url):
    """Extrahuje odkazy na vÅ¡echny obce na strÃ¡nce, i kdyÅ¾ jsou v rÅ¯znÃ½ch sloupcÃ­ch a tabulkÃ¡ch."""
    soup = naÄti_url(hlavnÃ­_url)
    tabulky = soup.find_all("table", class_="table")  # Najdeme vÅ¡echny tabulky

    if not tabulky:
        raise ValueError("âŒ Chyba: NepodaÅ™ilo se najÃ­t tabulky se seznamem obcÃ­.")

    odkazy = []
    for tabulka in tabulky:
        for Å™Ã¡dek in tabulka.find_all("tr")[2:]:  # PÅ™eskoÄÃ­me hlaviÄku
            buÅˆky = Å™Ã¡dek.find_all("td")
            if len(buÅˆky) < 2:
                continue  # PÅ™eskoÄÃ­me neplatnÃ© Å™Ã¡dky

            # Zpracujeme vÅ¡echny sloupce, ne jen prvnÃ­
            for i in range(0, len(buÅˆky), 3):  # PoÄÃ­tÃ¡me se tÅ™emi sloupci: ÄÃ­slo obce, nÃ¡zev, odkaz na okrsky
                if i + 1 >= len(buÅˆky):  # OvÄ›Å™Ã­me, Å¾e existuje dostatek bunÄ›k
                    continue
                kÃ³d = buÅˆky[i].text.strip()
                nÃ¡zev = buÅˆky[i+1].text.strip()
                odkaz = buÅˆky[i].find("a")
                
                if odkaz:
                    relativnÃ­_url = odkaz["href"]
                    plnÃ¡_url = f"https://www.volby.cz/pls/ps2017nss/{relativnÃ­_url}"
                    odkazy.append({"kÃ³d": kÃ³d, "nÃ¡zev": nÃ¡zev, "url": plnÃ¡_url})

    return odkazy

def extrahuj_vÃ½sledky_voleb(obec_url):
    """Extrahuje volebnÃ­ vÃ½sledky pro danou obec."""
    soup = naÄti_url(obec_url)

    # ğŸ“Š Extrakce statistickÃ½ch ÃºdajÅ¯
    statistickÃ¡_tabulka = soup.find("table", {"id": "ps311_t1"})
    if not statistickÃ¡_tabulka:
        raise ValueError("âŒ Chyba: NepodaÅ™ilo se najÃ­t tabulku se statistickÃ½mi Ãºdaji.")

    statistickÃ½_Å™Ã¡dek = statistickÃ¡_tabulka.find_all("tr")[2]
    buÅˆky = statistickÃ½_Å™Ã¡dek.find_all("td")

    registrovanÃ­ = vyÄisti_ÄÃ­slo(buÅˆky[3].text)
    obÃ¡lky = vyÄisti_ÄÃ­slo(buÅˆky[4].text)
    platnÃ© = vyÄisti_ÄÃ­slo(buÅˆky[7].text)

    # ğŸ“Š Extrakce vÃ½sledkÅ¯ stran
    vÃ½sledky_stran = {}
    tabulky_stran = soup.find_all("table", class_="table")

    for tabulka in tabulky_stran:
        for Å™Ã¡dek in tabulka.find_all("tr")[2:]:
            buÅˆky = Å™Ã¡dek.find_all("td")
            if len(buÅˆky) < 3:
                continue

            nÃ¡zev_strany = buÅˆky[1].text.strip()
            hlasy = vyÄisti_ÄÃ­slo(buÅˆky[2].text)
            vÃ½sledky_stran[nÃ¡zev_strany] = hlasy

    return {
        "registrovanÃ­": registrovanÃ­,
        "obÃ¡lky": obÃ¡lky,
        "platnÃ©": platnÃ©,
        "strany": vÃ½sledky_stran
    }

def uloÅ¾_do_csv(vÃ½stupnÃ­_soubor, data):
    """UloÅ¾Ã­ vÃ½sledky voleb do CSV s oddÄ›lovaÄem stÅ™ednÃ­kem."""
    strany = list(data[0]["strany"].keys())

    hlaviÄka = ["kÃ³d", "obec", "registrovanÃ­", "obÃ¡lky", "platnÃ©"] + strany

    with open(vÃ½stupnÃ­_soubor, "w", newline="", encoding="utf-8-sig") as f:
        zapisovaÄ = csv.writer(f, delimiter=";")
        zapisovaÄ.writerow(hlaviÄka)

        for zÃ¡znam in data:
            Å™Ã¡dek = [
                zÃ¡znam["kÃ³d"],
                zÃ¡znam["obec"],
                zÃ¡znam["registrovanÃ­"],
                zÃ¡znam["obÃ¡lky"],
                zÃ¡znam["platnÃ©"]
            ] + [zÃ¡znam["strany"].get(strana, 0) for strana in strany]
            zapisovaÄ.writerow(Å™Ã¡dek)

def hlavnÃ­():
    """HlavnÃ­ funkce pro scraping."""
    url = input("ğŸ”— Zadejte URL pro scraping: ").strip()
    vÃ½stupnÃ­_soubor = input("ğŸ’¾ Zadejte nÃ¡zev vÃ½stupnÃ­ho CSV souboru (napÅ™. vysledky.csv): ").strip()

    print(f"\nğŸ“Œ ZpracovÃ¡vÃ¡m data z URL: {url}")
    
    try:
        obce = extrahuj_odkazy_na_obce(url)
        vÅ¡echny_vÃ½sledky = []

        for obec in obce:
            print(f"ğŸ“ ZpracovÃ¡vÃ¡m obec: {obec['nÃ¡zev']} (kÃ³d: {obec['kÃ³d']})")
            try:
                vÃ½sledek = extrahuj_vÃ½sledky_voleb(obec["url"])
                vÃ½sledek["kÃ³d"] = obec["kÃ³d"]
                vÃ½sledek["obec"] = obec["nÃ¡zev"]
                vÅ¡echny_vÃ½sledky.append(vÃ½sledek)
            except Exception as e:
                print(f"âŒ Chyba pÅ™i zpracovÃ¡nÃ­ obce {obec['nÃ¡zev']}: {e}")

        uloÅ¾_do_csv(vÃ½stupnÃ­_soubor, vÅ¡echny_vÃ½sledky)
        print(f"\nâœ… VÃ½sledky byly ÃºspÄ›Å¡nÄ› uloÅ¾eny do souboru {vÃ½stupnÃ­_soubor}")

    except Exception as e:
        print(f"âŒ Chyba: {e}")

if __name__ == "__main__":
    hlavnÃ­()
